# -*- coding: utf-8 -*-
"""Image_Segmentation_with_PyTorch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X-aa6P2Y6PxkmAIIJhcA69bwLmWRD26b

# Introduction

##  Objective
The goal of this project is to implement and compare three popular deep learning architectures for image segmentation: UNet, FPN (Feature Pyramid Network), and DeepLabV3+. We aim to evaluate their performance on a human segmentation dataset in terms of segmentation accuracy, Dice coefficient, IoU, including total training time and resource usage.

# **Step 1: Install Required Libraries und DATA**
"""

!pip install segmentation-models-pytorch
!pip install -U git+https://github.com/albumentations-team/albumentations
!pip install --upgrade opencv-contrib-python

"""# Installation Dataset
---

* Dataset Name: Human Segmentation Dataset


* Content: RGB images + corresponding binary masks

* Mask Meaning:

  1 → human present

  0 → background / human absent

* Number of Samples: 290

"""

!git clone https://github.com/parth1620/Human-Segmentation-Dataset-master.git

"""# **Step 2: Import Libraries**"""

import torch
import cv2

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from tqdm import tqdm

import albumentations as A

from torch.utils.data import Dataset
from torch.utils.data import DataLoader

from torch import nn
import segmentation_models_pytorch as smp
from segmentation_models_pytorch.losses import DiceLoss

import sys
sys.path.append('/content/Human-Segmentation-Dataset-master')

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

"""

# **Step 3: Data Preparation**


*  Loading the dataset (train / validation)

"""

CSV_FILE = '/content/Human-Segmentation-Dataset-master/train.csv'
DATA_DIR = '/content/'

DEVICE ='cuda'

EPOCHS = 25
LR = 0.003
IMAGE_SIZE = 320
BATCH_SIZE = 16

ENCODER ='timm-efficientnet-b0'
WEIGHTS = 'imagenet'

df = pd.read_csv(CSV_FILE)
df.head()

print(f"Total images: {len(df)}")

train_df , valid_df = train_test_split(df, test_size = 0.2 , random_state = 42)

"""

* Visualizing sample images and masks





"""

def show_image_and_mask(image, mask,pred_image = None, title_image="Image", title_mask="Mask"):
  if pred_image == None:
    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))

    ax1.set_title(title_image)  # Titre du premier subplot
    ax1.imshow(image)       # Affiche l'image originale

    ax2.set_title(title_mask)  # Titre du deuxième subplot
    ax2.imshow(mask,cmap = 'gray')  # Affiche le masque en niveaux de gris

  elif pred_image != None :

        f, (ax1, ax2,ax3) = plt.subplots(1, 3, figsize=(10,5))

        ax1.set_title('IMAGE')
        ax1.imshow(image.permute(1,2,0).squeeze(),cmap = 'gray')

        ax2.set_title('GROUND TRUTH')
        ax2.imshow(mask.permute(1,2,0).squeeze(),cmap = 'gray')

        ax3.set_title('MODEL OUTPUT')
        ax3.imshow(pred_image.permute(1,2,0).squeeze(),cmap = 'gray')

# Select a specific row from the DataFrame (here the 33rd row as index starts at 0)
row = df.iloc[32]

# Get the image and mask paths
image_path = row.images
mask_path = row.masks
image = cv2.imread(image_path)
# Convert BGR → RGB because matplotlib and models use RGB.
image= cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)/ 255.0

show_image_and_mask(image, mask)

"""* Explanation of the data format (RGB images + binary masks)



"""

print("Image shape:", image.shape)
print("Mask shape:", mask.shape)
print("Mask unique values:", np.unique(mask))

"""

# **Step 4: Data Augmentation**

* We use the albumentations library to apply data augmentation to our training dataset. Augmentation helps improve the generalization of the model, especially when the dataset is small.


"""

def get_train_augs():
  return A.Compose([
      A.Resize(IMAGE_SIZE, IMAGE_SIZE),
      A.HorizontalFlip(p = 0.5),
      A.VerticalFlip(p = 0.5)
      ], is_check_shapes=False)
def get_valid_augs():
  return A.Compose([
      A.Resize(IMAGE_SIZE, IMAGE_SIZE)
      ], is_check_shapes=False)

"""* Example: Image before and after augmentation




"""

# Original image and mask
show_image_and_mask(image, mask)

# Apply augmentation
augs = get_train_augs()
augmented = augs(image=image, mask=mask)
image_aug = augmented['image']
mask_aug = augmented['mask']

# Display augmented image and mask
show_image_and_mask(image_aug, mask_aug,title_image="image augmentée",title_mask="mask Augmentée")

"""# **Step 5: Creating the Dataset and DataLoader**

* Definition of the custom SegmentationDataset class




"""

class SegmentationDataset(Dataset):
  def __init__(self, df, augmentations):
    self.df = df
    self.augmentations = augmentations

  def __len__(self):
    return len(self.df)

  def __getitem__(self, idx):

    row = self.df.iloc[idx]

    image_path = row.images
    mask_path = row.masks

    # Lecture
    image = cv2.imread(image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)

    mask = np.expand_dims(mask, axis = -1)


    # Augmentations
    if self.augmentations:
      data = self.augmentations(image=image, mask=mask)
      image = data['image']
      mask = data['mask']

    # (h, w, c) -> (c, h, w)
    image = np.transpose(image, (2, 0, 1)).astype(np.float32)
    mask = np.transpose(mask, (2, 0, 1)).astype(np.float32)

    # Conversion en Tensor
    image = torch.tensor(image) / 255.0

    mask = torch.tensor(mask)
    mask = torch.round(mask / 255.0)
    return image, mask

trainset = SegmentationDataset(train_df, get_train_augs())
validset = SegmentationDataset(valid_df, get_valid_augs())

print(f"Size of Trainset : {len(trainset)}")
print(f"Size of Validset : {len(validset)}")

"""
* Loading the training and validation batches using the DataLoader

"""

trainloader = DataLoader(trainset, batch_size= BATCH_SIZE, shuffle = True)
validloader = DataLoader(validset, batch_size= BATCH_SIZE)

print(f"total on , of batches in trainloadder ; {len(trainloader)}")
print(f"total on , of batches in validloadder ; {len(validloader)}")

"""* Checking that the data pipeline works correctly

"""

for images, masks in trainloader:
    break

print(f"One batch image shape : {images.shape}")
print(f"One batch mask shape : {masks.shape}")

"""# **Step 6: Building the Segmentation Model**

* Implementation of the SegmentationModel class

*  Model Architecture Selection (U-Net, FPN, DeepLabV3+)

*  Loss Function Definition (Dice Loss + Binary Cross-Entropy)

"""

class SegmentationModel(nn.Module):
    def __init__(self, arch='unet'):  # arch ici contrôle l’architecture
        super(SegmentationModel, self).__init__()

        self.arc = None

        if arch == 'unet':
            self.arc = smp.Unet(
                encoder_name=ENCODER,
                encoder_weights=WEIGHTS,
                in_channels=3,
                classes=1,
                activation=None
            )
        elif arch == 'fpn':
            self.arc = smp.FPN(
                encoder_name=ENCODER,
                encoder_weights=WEIGHTS,
                in_channels=3,
                classes=1,
                activation=None
            )
        elif arch == 'deeplabv3+':
            self.arc = smp.DeepLabV3Plus(
                encoder_name=ENCODER,
                encoder_weights=WEIGHTS,
                in_channels=3,
                classes=1,
                activation=None
            )
        else:
            raise ValueError(f"Architecture {arch} not supported")

    def forward(self, images, masks=None):
        logits = self.arc(images)
        if masks is not None:
            loss1 = DiceLoss(mode='binary')(logits, masks)
            loss2 = nn.BCEWithLogitsLoss()(logits, masks)
            return logits, loss1 + loss2
        return logits

model_unet = SegmentationModel(arch='unet')
model_unet.to(DEVICE);

model_fpn = SegmentationModel(arch='fpn').to(DEVICE)

model_deeplab = SegmentationModel(arch='deeplabv3+').to(DEVICE)

"""# **Step 7: Model Training**

* Training and validation loops


"""

def train_fn(data_loader, model, optimizer):
  model.train()
  total_loss = 0.0

  for images , masks in tqdm(data_loader):

    images = images.to(DEVICE)
    masks = masks.to(DEVICE)

    optimizer.zero_grad()
    logits, loss = model(images, masks)
    loss.backward()
    optimizer.step()

    total_loss += loss.item()

  return total_loss / len(data_loader)

def eval_fn(data_loader, model):
  model.eval()
  total_loss = 0.0

  with torch.no_grad():
    for images , masks in tqdm(data_loader):

      images = images.to(DEVICE)
      masks = masks.to(DEVICE)


      logits, loss = model(images, masks)


      total_loss += loss.item()

  return total_loss / len(data_loader)

"""* Computation of losses and metrics (IoU, Dice score)

* Saving the best-performing model based on validation loss

###   **model_unet**
"""

import time
optimizer_unet  = torch.optim.Adam(model_unet.parameters(),lr = LR)

best_valid_loss_unet = np.inf

start_time = time.time()
for i in range(EPOCHS):
  train_loss = train_fn(trainloader, model_unet, optimizer_unet)
  valid_loss = eval_fn(validloader, model_unet)


  if valid_loss < best_valid_loss_unet:
    torch.save(model_unet.state_dict(),'best_model_unet.pt')
    print("SAVED-MODEL")
    best_valid_loss_unet = valid_loss

  print(f"Epoch : {i+1} train_loss : {train_loss} valid_loss : {valid_loss}")

end_time = time.time()
print(f"Time taken: {end_time - start_time:.2f} seconds")

"""### **model_fpn**"""

optimizer_fpn = torch.optim.Adam(model_fpn.parameters(),lr = LR)

best_valid_loss_fpn = np.inf

start_time = time.time()
for i in range(EPOCHS):

  train_loss = train_fn(trainloader, model_fpn, optimizer_fpn)
  valid_loss = eval_fn(validloader, model_fpn)

  if valid_loss < best_valid_loss_fpn:
    torch.save(model_fpn.state_dict(),'best_model_fpn.pt')
    print("SAVED-MODEL")
    best_valid_loss_fpn = valid_loss

  print(f"Epoch : {i+1} train_loss : {train_loss} valid_loss : {valid_loss}")

end_time = time.time()
print(f"Time taken: {end_time - start_time:.2f} seconds")

"""### **model_deeplab**"""

optimizer_deeplab = torch.optim.Adam(model_fpn.parameters(),lr = LR)

best_valid_loss_deeplab = np.inf

start_time = time.time()
for i in range(EPOCHS):

  train_loss = train_fn(trainloader, model_deeplab, optimizer_deeplab)
  valid_loss = eval_fn(validloader, model_deeplab)

  if valid_loss < best_valid_loss_deeplab:
    torch.save(model_deeplab.state_dict(),'best_model_deeplab.pt')
    print("SAVED-MODEL")
    best_valid_loss_deeplab = valid_loss

  print(f"Epoch : {i+1} train_loss : {train_loss} valid_loss : {valid_loss}")

end_time = time.time()
print(f"Time taken: {end_time - start_time:.2f} seconds")

"""# **Step 8: Evaluation and Results**
* Visualizationof predictions on validation images
* Comparison of performance between the architectures
*  Discussion of the results
"""

import torch

def compute_metrics(pred_mask, true_mask, threshold=0.5):
    """
    pred_mask : torch.Tensor (B, 1, H, W), logits ou probabilities
    true_mask : torch.Tensor (B, 1, H, W), valeurs 0 ou 1
    """
    pred_mask = (torch.sigmoid(pred_mask) > threshold).float()
    true_mask = true_mask.float()

    # Flatten
    pred_flat = pred_mask.view(-1)
    true_flat = true_mask.view(-1)

    # True Positives, False Positives, False Negatives
    TP = (pred_flat * true_flat).sum()
    FP = (pred_flat * (1 - true_flat)).sum()
    FN = ((1 - pred_flat) * true_flat).sum()

    # Dice
    dice = (2 * TP) / (2 * TP + FP + FN + 1e-8)

    # IoU
    iou = TP / (TP + FP + FN + 1e-8)

    return dice.item(), iou.item()

def eval_metrics(data_loader, model):
    model.eval()
    total_dice = 0
    total_iou = 0
    total_acc = 0
    total_pixels = 0

    with torch.no_grad():
        for images, masks in tqdm(data_loader):
            images = images.to(DEVICE)
            masks = masks.to(DEVICE)

            logits = model(images)

            dice, iou = compute_metrics(logits, masks)
            total_dice += dice
            total_iou += iou

            # Accuracy
            pred_mask = (torch.sigmoid(logits) > 0.5).float()
            correct = (pred_mask == masks).sum().item()
            total_acc += correct
            total_pixels += masks.numel()

    n_batches = len(data_loader)
    avg_dice = total_dice / n_batches
    avg_iou = total_iou / n_batches
    avg_acc = total_acc / total_pixels

    return avg_dice, avg_iou, avg_acc

#Load the best UNet model
model_unet.load_state_dict(torch.load('best_model_unet.pt'))
model_unet.to(DEVICE)


#Load the best Fpn model
model_fpn.load_state_dict(torch.load('best_model_fpn.pt'))
model_fpn.to(DEVICE)

#Load the best deeplab model
model_deeplab.load_state_dict(torch.load('best_model_deeplab.pt'))
model_deeplab.to(DEVICE);

# UNet
dice_unet, iou_unet, acc_unet = eval_metrics(validloader, model_unet)

# FPN
dice_fpn, iou_fpn, acc_fpn = eval_metrics(validloader, model_fpn)

# DeepLabV3+
dice_dl, iou_dl, acc_dl = eval_metrics(validloader, model_deeplab)

print("UNet     -> Dice: {:.4f}, IoU: {:.4f}, Acc: {:.4f}".format(dice_unet, iou_unet, acc_unet))
print("FPN      -> Dice: {:.4f}, IoU: {:.4f}, Acc: {:.4f}".format(dice_fpn, iou_fpn, acc_fpn))
print("DeepLabV3+ -> Dice: {:.4f}, IoU: {:.4f}, Acc: {:.4f}".format(dice_dl, iou_dl, acc_dl))

idx = 20

image, mask = validset[idx]

logits_mask_unet = model_unet(image.to(DEVICE).unsqueeze(0)) #(c,H,W) -> (1, C, H, W )

logits_mask_fpn = model_fpn(image.to(DEVICE).unsqueeze(0)) #(c,H,W) -> (1, C, H, W )

logits_mask_deeplab = model_deeplab(image.to(DEVICE).unsqueeze(0)) #(c,H,W) -> (1, C, H, W )

perd_mask_unet = torch.sigmoid(logits_mask_unet)
perd_mask_unet = (perd_mask_unet > 0.5)* 1.0

perd_mask_fpn = torch.sigmoid(logits_mask_fpn)
perd_mask_fpn = (perd_mask_fpn > 0.5)* 1.0

perd_mask_deeplab = torch.sigmoid(logits_mask_deeplab)
perd_mask_deeplab = (perd_mask_deeplab > 0.5)* 1.0

#Predicted mask by Unet
show_image_and_mask(image, mask,  perd_mask_unet.detach().cpu().squeeze(0),
                    title_image="Original Image", title_mask="Ground Truth")

#Predicted mask by FPN
show_image_and_mask(image, mask,  perd_mask_fpn.detach().cpu().squeeze(0),
                    title_image="FPN Prediction", title_mask="FPN Mask")

#Predicted mask by DeepLabV3+
show_image_and_mask(image, mask,  perd_mask_deeplab.detach().cpu().squeeze(0),
                    title_image="DeepLabV3+ Prediction", title_mask="DeepLab Mask")

import json

path = "/content/drive/MyDrive/Colab-Notebooks/Image_Segmentation_with_PyTorch.ipynb"

with open(path, "r", encoding="utf-8") as f:
    nb = json.load(f)

# Supprimer les widgets cassés
if "metadata" in nb and "widgets" in nb["metadata"]:
    del nb["metadata"]["widgets"]

# Sauvegarder la version propre
with open(path, "w", encoding="utf-8") as f:
    json.dump(nb, f, indent=2)

print("Notebook cleaned successfully!")

